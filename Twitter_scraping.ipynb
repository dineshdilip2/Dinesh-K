{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNVJ8vTJeJAER5HNcwj2rg9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dineshdilip2/Dinesh-K/blob/main/Twitter_scraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-2AI09kSKd6"
      },
      "outputs": [],
      "source": [
        "\n",
        "import snscrape.modules.twitter as tw\n",
        "import pandas as pd\n",
        "from pymongo import MongoClient\n",
        "import streamlit as st\n",
        "import csv \n",
        "import json\n",
        "\n",
        "\n",
        "# Define function to scrape Twitter data and store in MongoDB\n",
        "\n",
        "def scrape_twitter_data(keyword,start_date,end_date,tweet_limit,db):\n",
        "    tweets =[]\n",
        "    for tweet in tw.TwitterSearchScraper(f\"{keyword} since:{start_date} untill:{end_date}\").get_items():\n",
        "        if len(tweets)==tweet_limit:\n",
        "            break\n",
        "        tweets.append({\n",
        "            \"date\":tweet.date,\n",
        "            \"id\":tweet.id,\n",
        "            \"url\":tweet.url,\n",
        "            \"content\":tweet.content,\n",
        "            \"user\":tweet.user.username,\n",
        "            \"reply_count\":tweet.replyCount,\n",
        "            \"retweet_count\":tweet.retweetCount,\n",
        "            \"language\":tweet.lang,\n",
        "            \"source\":tweet.sourceLabel,\n",
        "            \"like_count\":tweet.likeCount\n",
        "         })\n",
        "        \n",
        "#Store the scraped data in Mongodb collection\n",
        "\n",
        "    collection = db[keyword]\n",
        "    collection.insert_one(tweets)\n",
        "    st.success(f\"{len(tweets)} tweets were scraped and saved to MongoDB.\")\n",
        "     \n",
        "    return pd.Dataframe(tweets)\n",
        "\n",
        "def export_data(data,filename,format):\n",
        "    if format == \"CSV\":\n",
        "        data.to_csv(filename,index=False)\n",
        "        st.success(f\"The data was exported to {filename} in CSV format.\")\n",
        "    elif format ==\"JSON\":\n",
        "        data.to_json(filename,orient= \"records\")\n",
        "        st.success(f\"The data was exported to {filename} in JSON format.\")\n",
        "    \n",
        "#connnect to MongoDB\n",
        "client = MongoClient(\"mongodb://localhost:27017/\")\n",
        "db = client[\"twitter_data\"]\n",
        "\n",
        "st.title(\"Twitter Data scraper\")\n",
        "\n",
        "keyword = st.text_input(\"Enter keyword or hashtag to search:\")\n",
        "start_date = st.date_input(\"Enter the start date:\")\n",
        "end_date =st.date_input(\"Ender end date:\")\n",
        "tweet_limit = st.number_input(\"Enter the number of tweets to scrape:\",min_value=1,max_value=10000)\n",
        "\n",
        "\n",
        "#scrape Twitter data and display in streamlit\n",
        "\n",
        "if st.button(\"Scrape Twitter Data\"):\n",
        "    data = scrape_twitter_data(keyword, str(start_date), str(end_date), int(tweet_limit), db)\n",
        "    st.write(data)\n",
        "    \n",
        "\n",
        "if st.button(\"Export Data\"):\n",
        "     data_format = st.selectbox(\"Select data format:\"[\"CSV\",\"JSON\"])  \n",
        "     filename = st.text_input(\"Enter filename:\")\n",
        "     export_data(data,filename,data_format)                         \n",
        "     "
      ]
    }
  ]
}